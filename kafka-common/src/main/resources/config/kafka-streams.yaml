# Generic Kafka Streams Configuration
properties:
  # Kafka bootstrap servers. Default to localhost:9092
  bootstrap.servers: ${kafka-streams.bootstrap.servers:localhost:9092}
  # Kafka key deserializer. Default to ByteArrayDeserializer
  key.deserializer: ${kafka-streams.key.deserializer:org.apache.kafka.common.serialization.ByteArrayDeserializer}
  # Kafka value deserializer. Default to ByteArrayDeserializer
  value.deserializer: ${kafka-streams.value.deserializer:org.apache.kafka.common.serialization.ByteArrayDeserializer}
  # Kafka auto offset reset. Default to earliest
  auto.offset.reset: ${kafka-streams.auto.offset.reset:earliest}
  # A unique application id for the Kafka streams app. You need to replace it or overwrite it in your code.
  application.id: ${kafka-streams.application.id:placeholder}
  # SSchema registry url
  schema.registry.url: ${kafka-streams.schema.registry.url:http://localhost:8081}
  # Schema registry auto register schema indicator for streams application. If true, the first request will register the schema auto automatically.
  schema.registry.auto.register.schemas: ${kafka-streams.schema.registry.auto.register.schemas:true}
  # Schema registry client truststore location, use the following two properties only if schema registry url is https.
  schema.registry.ssl.truststore.location: ${kafka-streams.schema.registry.ssl.truststore.location:/config/client.truststore}
  # Schema registry client truststore password
  schema.registry.ssl.truststore.password: ${kafka-streams.schema.registry.ssl.truststore.password:password}
  # security configuration for enterprise deployment
  security.protocol: ${kafka-streams.security.protocol:SASL_SSL}
  # SASL mechanism for authentication
  sasl.mechanism: ${kafka-streams.sasl.mechanism:PLAIN}
  # SASL JAAS configuration for authentication
  sasl.jaas.config: ${kafka-streams.sasl.jaas.config:org.apache.kafka.common.security.plain.PlainLoginModule required username="${kafka-streams.username:username}" password="${KAFKA_STREAMS_PASSWORD:password}";}
  # SSL truststore location for secure communication
  ssl.truststore.location: ${kafka-streams.ssl.truststore.location:/config/client.truststore}
  # SSL truststore password for secure communication
  ssl.truststore.password: ${kafka-streams.ssl.truststore.password:password}
  # SSL endpoint identification algorithm for secure communication. This is used to verify the hostname of the server against the certificate presented by the server.
  ssl.endpoint.identification.algorithm: ${kafka-streams.ssl.endpoint.identification.algorithm:also-name}
  # Client rack identifier for Kafka streams. Default to rack1
  client.rack: ${kafka-streams.client.rack:rack1}
  # basic authentication user:pass for the schema registry
  basic.auth.user.info: ${kafka-streams.username:username}:${KAFKA_STREAMS_PASSWORD:password}
  # basic authentication credentials source for the schema registry. Default to USER_INFO
  basic.auth.credentials.source: ${kafka-streams.basic.auth.credentials.source:USER_INFO}
  # The directory where Kafka Streams state is stored. Default to /tmp
  state.dir: ${kafka-streams.state.dir:/tmp}
  # Any additional properties that are not defined in the schema can be added here.
  # This is useful for custom configurations that are not part of the standard Kafka streams properties.
  additionalKafkaProperties: ${kafka-streams.additionalKafkaProperties:}
# Only set to true right after the streams reset and start the server. Once the server is up, shutdown and change this to false and restart.
cleanUp: ${kafka-streams.cleanUp:false}
# Common configuration properties between active and reactive consumers
# Indicator if the dead letter topic is enabled.
deadLetterEnabled: ${kafka-streams.deadLetterEnabled:true}
# The extension of the dead letter queue(topic) that is added to the original topic to form the dead letter topic
deadLetterTopicExt: ${kafka-streams.deadLetterTopicExt:.dlq}
# If audit is enabled, the producer will send the audit message to the audit topic.
auditEnabled: ${kafka-streams.auditEnabled:true}
# Audit log destination topic or logfile. Default to topic
auditTarget: ${kafka-streams.auditTarget:logfile}
# The consumer audit topic name if the auditTarget is topic
auditTopic: ${kafka-streams.auditTopic:sidecar-audit}
# The dead letter controller topic, one per environment.
deadLetterControllerTopic: ${kafka-streams.deadLetterControllerTopic:dev.ent.all.kafka.replay.metadata.0}

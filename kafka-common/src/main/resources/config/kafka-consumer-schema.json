{
  "$schema" : "http://json-schema.org/draft-07/schema#",
  "type" : "object",
  "required" : [ "properties", "deadLetterEnabled", "deadLetterTopicExt", "auditEnabled", "auditTarget", "auditTopic", "useNoWrappingAvro", "topic", "keyFormat", "valueFormat", "waitPeriod", "backendApiHost", "backendApiPath", "maxConsumerThreads", "serverId", "requestMaxBytes", "requestTimeoutMs", "fetchMinBytes", "instanceTimeoutMs", "iteratorBackoffMs", "backendConnectionReset", "maxRetries", "retryDelayMs", "batchRollbackThreshold" ],
  "properties" : {
    "properties" : {
      "type" : "object",
      "description" : "Generic Kafka Consumer Configuration",
      "properties" : {
        "bootstrap.servers" : {
          "type" : "string",
          "description" : "Kafka bootstrap servers. Default to localhost:9092",
          "default" : "localhost:9092"
        },
        "key.deserializer" : {
          "type" : "string",
          "description" : "Consumer will use the schema for deserialization from byte array\nKafka key deserializer. Default to ByteArrayDeserializer",
          "default" : "org.apache.kafka.common.serialization.ByteArrayDeserializer"
        },
        "value.deserializer" : {
          "type" : "string",
          "description" : "Kafka value deserializer. Default to ByteArrayDeserializer",
          "default" : "org.apache.kafka.common.serialization.ByteArrayDeserializer"
        },
        "enable.auto.commit" : {
          "type" : "boolean",
          "description" : "As the control pane or API to access admin endpoint for commit, this value should be false.",
          "default" : false
        },
        "auto.offset.reset" : {
          "type" : "string",
          "description" : "Kafka auto offset reset. Default to earliest",
          "default" : "earliest"
        },
        "group.id" : {
          "type" : "string",
          "description" : "Kafka consumer group id. Default to group1",
          "default" : "group1"
        },
        "schema.registry.url" : {
          "type" : "string",
          "description" : "Schema registry url",
          "default" : "http://localhost:8081"
        },
        "schema.registry.auto.register.schemas" : {
          "type" : "boolean",
          "description" : "Schema registry auto register schema indicator for streams application. If true, the first request will register the schema auto automatically.",
          "default" : true
        },
        "schema.registry.ssl.truststore.location" : {
          "type" : "string",
          "description" : "Schema registry client truststore location, use the following two properties only if schema registry url is https.",
          "default" : "/config/client.truststore"
        },
        "schema.registry.ssl.truststore.password" : {
          "type" : "string",
          "description" : "Schema registry client truststore password",
          "default" : "password"
        },
        "security.protocol" : {
          "type" : "string",
          "description" : "security configuration for enterprise deployment",
          "default" : "SASL_SSL"
        },
        "sasl.mechanism" : {
          "type" : "string",
          "description" : "SASL mechanism for authentication",
          "default" : "PLAIN"
        },
        "sasl.jaas.config" : {
          "type" : "string",
          "description" : "SASL JAAS configuration for authentication",
          "default" : "org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${kafka-consumer.username:username}\\\" password=\\\"${kafka-consumer.password:password}\\\";"
        },
        "ssl.truststore.location" : {
          "type" : "string",
          "description" : "SSL truststore location for secure communication",
          "default" : "/config/client.truststore"
        },
        "ssl.truststore.password" : {
          "type" : "string",
          "description" : "SSL truststore password for secure communication",
          "default" : "password"
        },
        "ssl.endpoint.identification.algorithm" : {
          "type" : "string",
          "description" : "SSL endpoint identification algorithm for secure communication. This is used to verify the hostname of the server against the certificate presented by the server.",
          "default" : "also-name"
        },
        "client.rack" : {
          "type" : "string",
          "description" : "Client rack identifier for Kafka consumer. Default to rack1",
          "default" : "rack1"
        },
        "basic.auth.user.info" : {
          "type" : "string",
          "description" : "basic authentication user:pass for the schema registry",
          "default" : "${kafka-consumer.username:username}:${KAFKA_CONSUMER_PASSWORD:password}"
        },
        "basic.auth.credentials.source" : {
          "type" : "string",
          "description" : "basic authentication credentials source for the schema registry. Default to USER_INFO",
          "default" : "USER_INFO"
        },
        "fetch.max.bytes" : {
          "type" : "number",
          "description" : "Max fetch size from Kafka cluster. Default 50mb is too big for cache consumption on the sidecar",
          "default" : 102400.0,
          "format" : "float32"
        },
        "max.poll.records" : {
          "type" : "number",
          "description" : "max poll records default is 500. Adjust it based on the size of the records to make sure each poll\nis similar to requestMaxBytes down below.",
          "default" : 100.0,
          "format" : "float32"
        },
        "max.partition.fetch.bytes" : {
          "type" : "number",
          "description" : "The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer.\nIf the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress.",
          "default" : 100.0,
          "format" : "float32"
        },
        "additionalKafkaProperties" : {
          "type" : "object",
          "description" : "Any additional kafka properties that are not defined in the schema can be added here.\nThis is useful for custom configurations that are not part of the standard Kafka consumer properties.",
          "additionalProperties" : {
            "type" : "string"
          }
        }
      }
    },
    "deadLetterEnabled" : {
      "type" : "boolean",
      "description" : "Common configuration properties between active and reactive consumers\nIndicator if the dead letter topic is enabled.",
      "default" : true
    },
    "deadLetterTopicExt" : {
      "type" : "string",
      "description" : "The extension of the dead letter queue(topic) that is added to the original topic to form the dead letter topic",
      "default" : ".dlq"
    },
    "auditEnabled" : {
      "type" : "boolean",
      "description" : "Indicator if the audit is enabled.",
      "default" : true
    },
    "auditTarget" : {
      "type" : "string",
      "description" : "Audit log destination topic or logfile. Default to topic"
    },
    "auditTopic" : {
      "type" : "string",
      "description" : "The consumer audit topic name if the auditTarget is topic",
      "default" : "logfile"
    },
    "useNoWrappingAvro" : {
      "type" : "boolean",
      "description" : "Indicate if the NoWrapping Avro converter is used. This should be used for avro schema with data type in JSON.",
      "default" : false
    },
    "topic" : {
      "type" : "string",
      "description" : "Reactive Consumer Specific Configuration\nThe topic that is going to be consumed. For reactive consumer only in the kafka-sidecar.\nIf two or more topics are going to be subscribed, concat them with comma without space.\ntopic: sidecar-test",
      "default" : "test1"
    },
    "keyFormat" : {
      "type" : "string",
      "description" : "the format of the key optional",
      "default" : "jsonschema"
    },
    "valueFormat" : {
      "type" : "string",
      "description" : "the format of the value optional",
      "default" : "jsonschema"
    },
    "waitPeriod" : {
      "type" : "string",
      "description" : "Waiting period in millisecond to poll another batch",
      "default" : "100"
    },
    "backendApiHost" : {
      "type" : "string",
      "description" : "Backend API host",
      "default" : "https://localhost:8444"
    },
    "backendApiPath" : {
      "type" : "string",
      "description" : "Backend API path",
      "default" : "/kafka/records"
    },
    "maxConsumerThreads" : {
      "type" : "number",
      "description" : "Active Consumer Specific Configuration and the reactive consumer also depends on these properties\ndefault max consumer threads to 50.",
      "default" : 50.0,
      "format" : "float32"
    },
    "serverId" : {
      "type" : "string",
      "description" : "a unique id for the server instance, if running in a Kubernetes cluster, use the container id environment variable",
      "default" : "id"
    },
    "requestMaxBytes" : {
      "type" : "number",
      "description" : "maximum number of bytes message keys and values returned. Default to 100*1024",
      "default" : 102400.0,
      "format" : "float32"
    },
    "requestTimeoutMs" : {
      "type" : "number",
      "description" : "The maximum total time to wait for messages for a request if the maximum number of messages hs not yet been reached.",
      "default" : 1000.0,
      "format" : "float32"
    },
    "fetchMinBytes" : {
      "type" : "number",
      "description" : "Minimum bytes of records to accumulate before returning a response to a consumer request. Default 10MB",
      "default" : -1.0,
      "format" : "float32"
    },
    "instanceTimeoutMs" : {
      "type" : "number",
      "description" : "amount of idle time before a consumer instance is automatically destroyed. If you use the ActiveConsumer and do not\nwant to recreate the consumer instance for every request, increase this number to a bigger value. Default to 5 minutes\nthat is in sync with max.poll.interval.ms default value. When this value is increased to a value greater than 5 minutes,\nthe max.poll.interval.ms will be automatically increased as these two values are related although completely different.",
      "default" : 300000.0,
      "format" : "float32"
    },
    "iteratorBackoffMs" : {
      "type" : "number",
      "description" : "Amount of time to backoff when an iterator runs out of date.",
      "default" : 50.0,
      "format" : "float32"
    },
    "backendConnectionReset" : {
      "type" : "boolean",
      "description" : "In case of .NET application we realized , under load, response comes back for batch HTTP request however FinACK does not come until\nkeep alive time out occurs and sidecar consumer does not move forward. Hence we are adding this property so that we can explicitly close the connection\nwhen we receive the response and not wait for FinAck.",
      "default" : false
    },
    "maxRetries" : {
      "type" : "number",
      "description" : "Max retries when exception occurs.",
      "default" : 3.0,
      "format" : "float32"
    },
    "retryDelayMs" : {
      "type" : "number",
      "description" : "Delay milliseconds between retries.",
      "default" : 1000.0,
      "format" : "float32"
    },
    "batchRollbackThreshold" : {
      "type" : "number",
      "description" : "The percentage threshold (0–100) of records in a batch that are allowed to fail before the entire batch is rolled back.\n  - 0  = strict behavior: any failure rolls back the whole batch (nothing goes to DLQ for that batch).\n  - 100 = permissive behavior: the entire batch can fail and be sent to the DLQ without rollback.\nThe default of 30 is a conservative compromise: up to 30% of records may be redirected to the DLQ\nwhile still committing the successful records, which limits reprocessing of good messages but avoids\nsilently accepting mostly-bad batches. Adjust this value based on your tolerance for partial failures:\n  - Lower values (e.g. 5–10) for highly critical data where most failures should block the batch.\n  - Higher values (e.g. 50–80) for noisy or less critical topics where partial DLQing is acceptable.\nNOTE: Values outside 0–100 are considered invalid and may be rejected by the application or lead to undefined behavior.\n",
      "default" : 30.0,
      "format" : "float32"
    }
  }
}
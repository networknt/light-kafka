# Generic Kafka Consumer Configuration
properties:
  bootstrap.servers: ${kafka-consumer.bootstrap.servers:localhost:9092}
  # Consumer will use the schema for deserialization from byte array
  key.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
  value.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
  # As the control pane or API to access admin endpoint for commit, this value should be false.
  enable.auto.commit: false
  auto.offset.reset: ${kafka-consumer.auto.offset.reset:earliest}
  group.id: ${kafka-consumer.group.id:group1}
  schema.registry.url: ${kafka-consumer.schema.registry.url:http://localhost:8081}
  # security configuration for enterprise deployment
  # security.protocol: ${kafka-consumer.security.protocol:SASL_SSL}
  # sasl.mechanism: ${kafka-consumer.sasl.mechanism:PLAIN}
  # sasl.jaas.config: "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${kafka-consumer.username:username}\" password=\"${kafka-consumer.password:password}\";"
  # ssl.endpoint.identification.algorithm: ${kafka-consumer.ssl.endpoint.identification.algorithm:algo-name}
  # client.rack: ${kafka-consumer.client.rack:rack-name}
  # basic authentication user:pass for the schema registry
  # basic.auth.user.info: ${kafka-consumer.username:username}:${kafka-consumer.password:password}
  # basic.auth.credentials.source: ${kafka-consumer.basic.auth.credentials.source:USER_INFO}

# common configuration properties between active and reactive consumers
# The extension of the dead letter queue(topic) that is added to the original topic to form the dead letter topic
deadLetterTopicExt: ${kafka-consumer.deadLetterTopicExt:dlq}
# Indicator if the audit is enabled.
auditEnabled: ${kafka-consumer.auditEnabled:true}
# The consumer audit topic name
auditTopic: ${kafka-consumer.auditTopic:sidecar-audit}

# Reactive Consumer Specific Configuration
# The topic that is going to be consumed. For reactive consumer only in the kafka-sidecar.
# If two or more topics are going to be subscribed, concat them with comma without space.
# topic: sidecar-test
topic: ${kafka-consumer.topic:test1}
# the format of the key optional
keyFormat: ${kafka-consumer.keyFormat:jsonschema}
# the format of the value optional
valueFormat: ${kafka-consumer.valueFormat:jsonschema}
# Waiting period in millisecond to poll another batch
waitPeriod: ${kafka-consumer.waitPeriod:10000}
# Backend API host
backendApiHost: ${kafka-consumer.backendApiHost:https://localhost:8444}
# Backend API path
backendApiPath: ${kafka-consumer.backendApiPath:/kafka/records}

# Active Consumer Specific Configuration and the reactive consumer also depends on these properties
# default max consumer threads to 50.
maxConsumerThreads: ${kafka-consumer.maxConsumerThreads:50}
# a unique id for the server instance, if running in a Kubernetes cluster, use the container id environment variable
serverId: ${kafka-consumer.serverId:id}
# maximum number of bytes message keys and values returned. Default to 64*1024*1024
requestMaxBytes: ${kafka-consumer.requestMaxBytes:67108864}
# The maximum total time to wait for messages for a request if the maximum number of messages hs not yet been reached.
requestTimeoutMs: ${kafka-consumer.requestTimeoutMs:1000}
# Minimum bytes of records to accumulate before returning a response to a consumer request. Default 10MB
fetchMinBytes: ${kafka-consumer.fetchMinBytes:10000000}
# amount of idle time before a consumer instance is automatically destroyed.
instanceTimeoutMs: ${kafka-consumer.instanceTimeoutMs:300000}
# Amount of time to backoff when an iterator runs out of date.
iteratorBackoffMs: ${kafka-consumer.iteratorBackoffMs:50}
